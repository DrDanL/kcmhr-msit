{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Military Service Identification Tool\n",
    "\n",
    "Please see README file for more information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--env setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "#--env setup - import sklearn for different machine learning algorithms \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn import metrics\n",
    "\n",
    "#--env setup - more friendly plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "#--env setup - we need to import nltk elements if not already installed\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions \n",
    "\n",
    "This section declares help functions used by the Tool. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--paths\n",
    "base_tool_dir = 'PATH'\n",
    "\n",
    "def documentCleaning(document):\n",
    "    #We clean all document inputs to ensure noise is removed\n",
    "    stemmer = WordNetLemmatizer()\n",
    "    \n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', document)\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    return document\n",
    "    \n",
    "def wordRemoval(document):\n",
    "    # We remove common words which were annotated as possible causing confusion to the classifier\n",
    "    # Read in master keyword file. This has been developed by a team of annotators\n",
    "    # This data is not publicly accessible\n",
    "    word_list_removal = []\n",
    "    with open(base_tool_dir + 'excluded_terms.csv') as csvDataFile:\n",
    "        csvReader = csv.reader(csvDataFile)\n",
    "        for row in csvReader:\n",
    "            word_list_removal.append(row[0])\n",
    "\n",
    "    for x_word in word_list_removal:\n",
    "        document = re.sub(x_word, '', document)\n",
    "        \n",
    "    return document\n",
    "\n",
    "def stopword(string):\n",
    "    # Remove all stop words defined in NLTK \n",
    "    a= [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(a)\n",
    "\n",
    "def milatryTermCheck(document):\n",
    "    # We remove common words which were annotated as possible causing confusion to the classifier\n",
    "    # Read in master keyword file \n",
    "    word_list = []\n",
    "    with open(base_tool_dir + 'data//data//military_terms.csv') as csvDataFile:\n",
    "        csvReader = csv.reader(csvDataFile)\n",
    "        for row in csvReader:\n",
    "            word_list.append(row[0])\n",
    "\n",
    "    if any(word in document for word in word_list):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def getMilTerms():\n",
    "    # We remove common words which were annoted as possible causing confusion to the classifier\n",
    "    # Read in master keyword file. This data is not publicly accessible\n",
    "    word_list = []\n",
    "    with open(base_tool_dir + 'military_terms.csv') as csvDataFile:\n",
    "        csvReader = csv.reader(csvDataFile)\n",
    "        for row in csvReader:\n",
    "            word_list.append(row[0])\n",
    "            \n",
    "    return word_list\n",
    "    \n",
    "def milatryWeights(tdif, training_features):\n",
    "    # We apply a minor weight based on a set of annotated terms. \n",
    "    with open(base_tool_dir + 'weight_terms.csv') as csvDataFile:\n",
    "        csvReader = csv.reader(csvDataFile)\n",
    "        for row in csvReader:\n",
    "            print(row[0])\n",
    "            try:\n",
    "                position = tdif.vocabulary_[row[0]]\n",
    "                training_features[:, position] *= 1.1\n",
    "                print('Term present')\n",
    "            except KeyError:\n",
    "                print('No term')\n",
    "            \n",
    "    return training_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the pipeline\n",
    "\n",
    "This section builds the machine learning model for the detection of military service in our cohort. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data read in. We export data from CRIS which has been annotated into a .csv file\n",
    "# This data represents free text medical notes wrriten by a doctor\n",
    "df_attachment_data = pd.read_csv(base_tool_dir + 'dataset.csv')\n",
    "df_attachment_data.rename(columns = {'Attachment_Text':'Text'}, inplace = True)\n",
    "df_attachment_data = df_attachment_data.dropna(subset=['Text'])\n",
    "\n",
    "# Add internal reference\n",
    "for index, row in df_attachment_data.iterrows():\n",
    "    df_attachment_data.loc[index, 'annotation_loc'] = ('att-' + str(index) + \".txt\")\n",
    "    \n",
    "# Keep only annotated data\n",
    "df_attachment_data = df_attachment_data[df_attachment_data['annotated'] == 1]\n",
    "display(df_attachment_data['flag_veteran'].value_counts()) # count the labels\n",
    "\n",
    "# Clear and process data into usable format\n",
    "df_attachment_data['TextCleaned'] = ''\n",
    "for index, row in df_attachment_data.iterrows():\n",
    "    df_attachment_data.loc[index, 'TextCleaned'] = documentCleaning(row['Text'])\n",
    "    \n",
    "# Remove words of confusion\n",
    "for index, row in df_attachment_data.iterrows():\n",
    "    df_attachment_data.loc[index, 'TextCleaned'] = wordRemoval(row['TextCleaned'])\n",
    "    \n",
    "# Remove stop words\n",
    "for index, row in df_attachment_data.iterrows():\n",
    "    df_attachment_data.loc[index, 'TextCleaned'] = stopword(row['TextCleaned'])\n",
    "    \n",
    "display(df_attachment_data.head(5))\n",
    "\n",
    "# Generate label sets ready for training the classifier\n",
    "df_attachment_data['labels'] = np.where(df_attachment_data['flag_veteran'] == 1, 'Veteran', 'Civilian')\n",
    "df_attachment_data['category_id'] = df_attachment_data['labels'].factorize()[0]\n",
    "category_id_df = df_attachment_data[['labels', 'category_id']].drop_duplicates().sort_values('category_id')\n",
    "id_to_category = dict(category_id_df[['category_id', 'labels']].values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development \n",
    "\n",
    "In this section we train a classical decision tree based on a fitted matrix of TF-IDF features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and testing set generation\n",
    "X_train, X_test, y_train, y_test, indices_train, \n",
    "    indices_test = train_test_split(df_attachment_data['TextCleaned'], \n",
    "                                    df_attachment_data['category_id'], df_attachment_data.index, \n",
    "                                    random_state=0, test_size=0.40, shuffle=True)\n",
    "\n",
    "# Obtain the tfid vectors \n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=10, max_df=0.5, stop_words='english', max_features=500)\n",
    "\n",
    "# Fit our data (training)\n",
    "training_features = tfidf.fit_transform(X_train)\n",
    "training_features = milatryWeights(tfidf, training_features)\n",
    "testing_features = tfidf.transform(X_test)\n",
    "\n",
    "# Viz. the covabulary used in the tfid\n",
    "print(tfidf.vocabulary_)\n",
    "pickle.dump(tfidf.vocabulary_,open(\"feature.pkl\",\"wb\"))\n",
    "\n",
    "# Model training - important that you may want to consider fine tuning these parameters\n",
    "model = DecisionTreeClassifier(criterion=\"entropy\", max_depth=5, random_state=0)\n",
    "model.fit(training_features, y_train)\n",
    "\n",
    "# Save the model to disk\n",
    "pickle.dump(model, open('finalized_model.sav', 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model\n",
    "\n",
    "This section tests the machine learning model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(testing_features)\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=category_id_df.labels.values, yticklabels=category_id_df.labels.values)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n",
    "\n",
    "for predicted in category_id_df.category_id:\n",
    "  for actual in category_id_df.category_id:\n",
    "    if predicted != actual and conf_mat[actual, predicted] >= 1:\n",
    "      print(\"'{}' predicted as '{}' : {} examples.\".format(id_to_category[actual], id_to_category[predicted], conf_mat[actual, predicted]))\n",
    "\n",
    "for predicted in category_id_df.category_id:\n",
    "  for actual in category_id_df.category_id:\n",
    "    if predicted != actual and conf_mat[actual, predicted] >= 1:\n",
    "      print(\"'{}' predicted as '{}' : {} examples.\".format(id_to_category[actual], id_to_category[predicted], conf_mat[actual, predicted]))\n",
    "\n",
    "df_attachment_data['model_outcome'] = ''\n",
    "df_attachment_data['model_outcome'].loc[indices_test] = y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post validation\n",
    "\n",
    "We add an extra layer of checks. All predictions are checked against a known list of military terms and phrases. If the record does not contain one of these known words. It is classed as not a military record. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred_rule = y_pred\n",
    "\n",
    "for idx, val in enumerate(y_pred_rule):\n",
    "    if(val == 1):\n",
    "        y_pred_rule[idx] = milatryTermCheck(df_attachment_data.loc[indices_test[idx]]['TextCleaned']) \n",
    "        \n",
    "df_attachment_data['rule_outcome'] = ''\n",
    "df_attachment_data['rule_outcome'].loc[indices_test] = y_pred_rule\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, y_pred_rule)\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=category_id_df.labels.values, yticklabels=category_id_df.labels.values)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n",
    "\n",
    "for predicted in category_id_df.category_id:\n",
    "  for actual in category_id_df.category_id:\n",
    "    if predicted != actual and conf_mat[actual, predicted] >= 1:\n",
    "      print(\"'{}' predicted as '{}' : {} examples.\".format(id_to_category[actual], id_to_category[predicted], conf_mat[actual, predicted]))\n",
    "\n",
    "for predicted in category_id_df.category_id:\n",
    "  for actual in category_id_df.category_id:\n",
    "    if predicted != actual and conf_mat[actual, predicted] >= 1:\n",
    "      print(\"'{}' predicted as '{}' : {} examples.\".format(id_to_category[actual], id_to_category[predicted], conf_mat[actual, predicted]))\n",
    "\n",
    "#--final decisions\n",
    "df_attachment_data.to_csv('output.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
